

% This executable is the main file from which all experiments are run. In
% summary, in this document a random sample of patients from the total
% number of patients who data has been obtained for are taken, and the
% indices of these patients is used to create a test set. The patient
% datasets associated with these indices are then removed from the training
% set. After this, the clustering algorithm is executed on the training
% feats and training labels, causing cluster-specific classifiers to be
% trained on each patient subset. Finally, the method
% "patient_ClusteringTest" is executed on the held out test set of
% patients, meaning that the test set patients become associated with
% clusters from the k means sorting, and the cluster-specific classifiers
% are used on them. Additionally, metrics are collected throughout,
% specifically at the end on the test set patients.

% Specific commentary will continue below.

%% Test Set Identification

%This loads the .mat data files, generated by
%"patient_clustering_pipeline".
clear all
load('BigOlLabels.mat')
load('BigOlFeats.mat')

%We begin by adding all features and labels to the training dataset
TrainFeats = BigOlFeats;
TrainLabels = BigOlLabels;

%A random subset of patients are selected to be in the test set.
index = [randsample(30,6)]'; 

TestSize = size(index,2);

%The randomly selected patients are added to a new test set, and then
%deleted from the training set.
for i = 1:size(index,2)
    TestFeats{index(i)} = TrainFeats{index(i)};
    TestLabels{index(i)} = TrainLabels{index(i)};
end

for i = 1:length(index)
    TrainFeats{index(i)} = [];
    TrainLabels{index(i)} = [];
end

found_one = true;

while(found_one)
    for counter = 1:length(TrainFeats)
        found_one = false;
        if(isempty(TrainFeats{counter}))
             TrainFeats(counter) = [];
             TrainLabels(counter) = [];
             found_one = true;
             break;
        end
    end
end

%% Training

%Unsupervised Training. Before we train classifiers for each of the patient
%clusters, we have an opportunity to warm-up our clustering algorithm with
%patients whose seizure annotations we do not have. This is a simple call
%to the Data2Cluster method, which is described elsewhere. Its stack of
%features used for clustering is stored as a persistent variable. See
%Data2Cluster.m for details. The inde

Num_Clusters = 7;
%Data2Cluster(UnAnnotatedFeats, Num_Clusters, true)

%Supervised Training. The training set of patients is clustered, and cluster-specific classifier
%are trained on each cluster.

[ModelArray, Precision_Vec, Recall_Vec, patient_in_quant, IsolateInstanceCount] = patient_Clustering(TrainFeats, TrainLabels, Num_Clusters)


%%Testing

%We prepare to calculate our test statistics via use of mutable variables,
%which will first serve as the sums of their respective statistics, and
%then be divided by the number of test samples taken to find their actual
%values.
avgPrec = 0;
avgIsolateInstances = 0;
avgRecall = 0;
avgF1 = 0;
avgAdRecall = 0;
SmoothMax = 3;

for i = 1:TestSize
    %For each patient in the test set, they are clustered into one of the
    %training set clusters (but do not themselves affect the cluster
    %distribution) and then have their cluster-specific applied to make
    %predictions for them. The results of this classification are compared
    %against the true labels provided.
    [F1, Precision, Recall, IsolateInstances, Yhat] = patient_ClusteringTest(TestFeats{index(i)}, TestLabels{index(i)}, ModelArray, Num_Clusters);

    %Vote filtering is applied, which is the method of smoothing out
    %window-wise classifications described in "VoteFiltering.m".
    Ysmooth = Yhat; 
        for j = 3:SmoothMax
            [Ysmooth] = VoteFiltering(Ysmooth,j);
        end

    %Testing metrics are calculated.
    [IsolateInstances,TPrate,TNrate,Precision,Recall,F1,AdvRecall] = JustMetrics(Ysmooth,TestLabels{index(i)})
    avgPrec = Precision + avgPrec;
    avgIsolateInstances = IsolateInstances + avgIsolateInstances;
    avgRecall = Recall + avgRecall;
    avgF1 = F1 + avgF1;
    % AdvRecall = AdvancedRecallMeasure(Yhat,TestLabels{index(i)});
    avgAdRecall = AdvRecall + avgAdRecall;
end

%As mentioned above, the variables below were being used as counters, and
%now have their values divided so that they reflect true metric averages.
%This coul be simplified substantially.
avgPrec = avgPrec/TestSize;
avgIsolateInstances = avgIsolateInstances/TestSize;
avgRecall = avgRecall/TestSize;
avgF1 = avgF1/TestSize;
avgAdRecall = avgAdRecall/TestSize;
